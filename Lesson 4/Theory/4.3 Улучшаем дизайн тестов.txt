Отрицательные проверки: как проверить отсутствие элемента
Иногда в ходе написания авто-тестов возникает ситуация, когда нам нужно проверить не только присутствие элемента на странице, но и то, что элемента на странице нет. Здесь стоит разделять две принципиально разные ситуации, в зависимости от того как ведет себя веб-приложение: 
 1. Элемент потенциально может появится на странице (но вообще-то не должен). Например, мы открываем страницу товара, и ожидаем, что там нет сообщения об успешном добавлении в корзину. Мы проверяем, что элемента нет, но при позитивном сценарии, когда мы добавляем товар в корзину, сообщение тоже появляется не сразу. Если при негативной проверке мы не добавим ожидание, а сразу выдадим результат: "True, элемента действительно нет, все хорошо", мы рискуем нарваться на ложно-зеленый тест. То есть, можем пропустить баг. 
2. Элемент присутствует на странице и должен исчезнуть со временем или в результате действий пользователя. Это может быть, например, удаление товара из корзины, или исчезновение лоадера с загрузкой. 
Почему нужно писать такие проверки с осторожностью? 
Во-первых, нам приходится всегда гарантированно ждать. В первом примере нам всегда нужно ждать несколько секунд, чтобы убедиться, что элемент не появился. Если мы используем нашу написанную функцию is_element_present, то тест с такой проверкой будет ждать полные и честные 10 секунд:
def should_not_be_success_message(self):
    assert not self.is_element_present(*ProductPageLocators.SUCCESS_MESSAGE),\
        "Success message is presented"
Что очень много для зелёного теста. То есть implicit_wait уже в такой ситуации не подходит, придется использовать явное ожидание и аккуратно подбирать условия. Время ожидания тоже придется подбирать эмпирически, путем проб, ошибок, ложноположительных и ложноотрицательных результатов. 
Во-вторых, еще одна загвоздка с отрицательными проверками в том, что они могут давать ложноположительные срабатывания, если селектор устарел. Проверяем, что элемента с таким селектором нет, — проверка проходит, так как у элемента уже другой селектор. Элемент есть на экране — это баг, а тест зеленый. Это плохо! 
Поэтому на каждый негативный тест обязательно должен приходиться положительный тест. В одном тесте проверяем, что элемента нет, в соседнем тесте, что элемент есть. Тогда мы сможем отслеживать актуальность селектора и не пропустим такой баг. 
Дополнительная ссылка на пост Виталия Котова про распространенные грабли и отрицательные проверки, в том числе:
 UI-автотесты: как делать не стоит
Как же тогда реализовывать такие проверки? 
Нужно ориентироваться на конкретную ситуацию, но общий совет — использовать явные ожидания и Expected Conditions, о которых мы говорили в предыдущих модулях. 
Можно добавить в BasePage абстрактный метод, который проверяет, что элемент не появляется на странице в течение заданного времени: 
def is_not_element_present(self, how, what, timeout=4):
    try:
        WebDriverWait(self.browser, timeout).until(EC.presence_of_element_located((how, what)))
    except TimeoutException:
        return True

    return False
Тогда его использование Page Object для страницы товара будет выглядеть так: 
def should_not_be_success_message(self):
    assert self.is_not_element_present(*ProductPageLocators.SUCCESS_MESSAGE), \
       "Success message is presented, but should not be"
Если же мы хотим проверить, что какой-то элемент исчезает, то следует воспользоваться явным ожиданием вместе с функцией until_not, в зависимости от того, какой результат мы ожидаем: 
def is_disappeared(self, how, what, timeout=4):
    try:
        WebDriverWait(self.browser, timeout, 1, TimeoutException).\
            until_not(EC.presence_of_element_located((how, what)))
    except TimeoutException:
        return False

    return True
Метод-проверка в классе про страницу товара будет выглядеть аналогично should_not_be_success_message, напишите его самостоятельно.
  Обратите внимание на разницу между методами is_not_element_present и is_disappeared. 
is_not_element_present: упадет, как только увидит искомый элемент. Не появился: успех, тест зеленый. 
is_disappeared: будет ждать до тех пор, пока элемент не исчезнет. 
  Резюмируя, можно сказать, что разрабатывать такие проверки нужно очень аккуратно, использовать явные ожидания для сокращения времени прогона теста и всегда добавлять позитивную проверку на элемент в другом тесте. Без явной необходимости таких проверок лучше избегать. 


Группировка тестов: setup 
А сейчас воспользуемся магией ООП уже для организации кода самих тест-кейсов. PyTest позволяет объединять несколько тест-кейсов в один класс. Зачем это делать и почему удобно? 
Во-первых, мы можем логически сгруппировать тесты в один класс просто ради более стройного кода: удобно, когда тесты, связанные с одним компонентом лежат в одном классе, а с помощью pytest.mark можно помечать сразу весь класс. Основное правило такое: название класса должно начинаться с Test, чтобы PyTest смог его обнаружить и запустить.
Давайте например объединим в группу два теста в файле test_main_page.py и пометим его меткой login_guest:
@pytest.mark.login_guest
class TestLoginFromMainPage():
    # не забываем передать первым аргументом self                       
    def test_guest_can_go_to_login_page(self, browser):     
         # реализация теста

    def test_guest_should_see_login_link(self, browser):
         # реализация теста
Попробуйте запустить тесты в этом файле с меткой (нужно добавить "-m login_guest"). Вы увидите, что запустились оба теста, хотя метка всего одна. 
Во-вторых, для разных тест-кейсов можно выделять общие функции, чтобы не повторять код. Эти функции называются setup — функция, которая выполнится перед запуском каждого теста из класса, обычно туда входит подготовка данных, и teardown — функция, которая выполняется ПОСЛЕ каждого теста из класса, обычно там происходит удаление тех данных, которые мы создали во время теста. Хороший автотест должен сработать даже на чистой базе данных и удалить за собой сгенерированные в тесте данные. Такие функции реализуются с помощью фикстур, которые мы изучили в предыдущем модуле. Чтобы функция запускалась автоматически перед каждым тест-кейсом, нужно пометить её как @pytest.fixture с параметрами scope="function", что значит запускать на каждую функцию, и autouse=True, что значит запускать автоматически без явного вызова фикстуры.
Мы уже немного говорили про независимость от контента в предыдущих шагах — идеальным решением было бы везде, где мы работаем со страницей продукта, создавать новый товар в нашем интернет-магазине перед тестом и удалять по завершении теста. К сожалению, наш интернет-магазин пока не имеет возможности создавать объекты по API, но в идеальном мире мы бы написали вот такой тест-класс в файле test_product_page.py:
@pytest.mark.login
class TestLoginFromProductPage():
    @pytest.fixture(scope="function", autouse=True)
    def setup(self):
        self.product = ProductFactory(title = "Best book created by robot")
        # создаем по апи
        self.link = self.product.link
        yield
        # после этого ключевого слова начинается teardown
        # выполнится после каждого теста в классе
        # удаляем те данные, которые мы создали 
        self.product.delete()
        

    def test_guest_can_go_to_login_page_from_product_page(self, browser):
        page = ProductPage(browser, self.link)
        # дальше обычная реализация теста

    def test_guest_should_see_login_link(self, browser):
        page = ProductPage(browser, self.link)
        # дальше обычная реализация теста
Работа с API выходит за рамки этого курса, но знание о том, что можно группировать тесты и выделять подготовительные шаги в единые для всех тестов функции — важно для каждого автоматизатора.


